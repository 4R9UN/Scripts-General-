{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_request_to_model(prompt):\n",
    "    url = \"http://100.79.97.9:123/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer YOUR_API_KEY\"  # Replace with your actual API key if required\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"hermes-3-llama-3.2-3b\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"An error occurred: {response.status_code} {response.reason}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import threading\n",
    "import requests\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import subprocess\n",
    "import platform\n",
    "from bs4 import BeautifulSoup\n",
    "from flask import Flask, render_template, request, jsonify, redirect\n",
    "from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n",
    "from typing import List\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI  # Adjust as needed\n",
    "from flask_caching import Cache\n",
    "\n",
    "# Create Flask app with a dynamic secret key.\n",
    "app = Flask(__name__)\n",
    "app.secret_key = os.urandom(24)\n",
    "\n",
    "# Configure caching (here using SimpleCache; for production, use Redis or similar)\n",
    "cache = Cache(app, config={'CACHE_TYPE': 'SimpleCache', 'CACHE_DEFAULT_TIMEOUT': 300})\n",
    "\n",
    "# Global variables for configuration.\n",
    "LM_Model = 'hermes-3-llama-3.2-3b'\n",
    "Embeddings_model = 'text-embedding-nomic-embed-text-v1.5'\n",
    "Server_Ip = '100.79.162.55'\n",
    "\n",
    "# ---------------------------\n",
    "# Custom Embedding Setup with Caching\n",
    "# ---------------------------\n",
    "class CustomLocalEmbedding(Embeddings):\n",
    "    def __init__(self, endpoint_url=f'http://{Server_Ip}:1234/v1/embeddings', model_name=Embeddings_model):\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.model_name = model_name\n",
    "\n",
    "    @cache.memoize()\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        payload = {\"model\": self.model_name, \"input\": text}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        response = requests.post(self.endpoint_url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" in data and isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0:\n",
    "                if \"embedding\" in data[\"data\"][0]:\n",
    "                    return data[\"data\"][0][\"embedding\"]\n",
    "            elif \"embedding\" in data:\n",
    "                return data[\"embedding\"]\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected response structure: \" + str(data))\n",
    "        else:\n",
    "            raise Exception(f\"Embedding request failed: {response.status_code} {response.text}\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "embedding_model = CustomLocalEmbedding()\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# ---------------------------\n",
    "# Global Settings\n",
    "# ---------------------------\n",
    "settings = {\n",
    "    \"vector_db_location\": \"vector_db\"  # Folder for storing the FAISS index.\n",
    "}\n",
    "\n",
    "# Global flag for KQL authentication.\n",
    "kql_authenticated = False\n",
    "\n",
    "# In-memory chat history.\n",
    "chat_history = {\"default\": []}\n",
    "\n",
    "# Global vector store variable.\n",
    "vector_store = None\n",
    "if os.path.exists(settings['vector_db_location']):\n",
    "    try:\n",
    "        vector_store = FAISS.load_local(settings['vector_db_location'], embedding_model)\n",
    "        print(\"Loaded vector store from disk.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading vector store:\", e)\n",
    "\n",
    "def add_document_to_vector_db(doc_text, metadata=None):\n",
    "    global vector_store\n",
    "    docs = text_splitter.split_text(doc_text)\n",
    "    from langchain.docstore.document import Document\n",
    "    documents = [Document(page_content=chunk, metadata=metadata or {}) for chunk in docs]\n",
    "    if vector_store is None:\n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "    else:\n",
    "        vector_store.add_documents(documents)\n",
    "    vector_store.save_local(settings['vector_db_location'])\n",
    "    return \"Document indexed successfully.\"\n",
    "\n",
    "# ---------------------------\n",
    "# LM Model Interaction Functions\n",
    "# ---------------------------\n",
    "def send_request_to_model(prompt, model=LM_Model):\n",
    "    url = f\"http://{Server_Ip}:1234/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer YOUR_API_KEY\"  # Update with your actual API key.\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} {response.reason}\")\n",
    "        return None\n",
    "\n",
    "def send_request_to_tuned_model(prompt, context=None):\n",
    "    full_prompt = prompt + ((\"\\n\\nContext:\\n\" + context) if context else \"\")\n",
    "    return send_request_to_model(full_prompt, model=\"tuned-hermes-3-llama-3.2-3b\")\n",
    "\n",
    "# ---------------------------\n",
    "# Document & URL Processing Functions\n",
    "# ---------------------------\n",
    "def process_uploaded_file(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == '.pdf':\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "        return text\n",
    "    elif ext == '.csv':\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return df.to_html(classes=\"table table-striped\")\n",
    "        except Exception as e:\n",
    "            return f\"Error processing CSV: {e}\"\n",
    "    else:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error reading file: {e}\"\n",
    "\n",
    "def process_url_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup.get_text(separator=\"\\n\")\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching URL: {e}\"\n",
    "\n",
    "# ---------------------------\n",
    "# KQL Query Functions with Caching for Similar Queries\n",
    "# ---------------------------\n",
    "def generate_kql_explanation(query, results):\n",
    "    results_text = json.dumps(results, indent=2, default=str)\n",
    "    prompt = f\"Summarize the following KQL query results and provide numerical insights:\\nQuery: {query}\\nResults:\\n{results_text}\"\n",
    "    response = send_request_to_model(prompt, model=\"tuned-hermes-3-llama-3.2-3b\")\n",
    "    if response and 'choices' in response and response['choices']:\n",
    "        return response['choices'][0]['message']['content']\n",
    "    else:\n",
    "        return \"No summary available.\"\n",
    "\n",
    "def execute_kql_query(query):\n",
    "    import re\n",
    "    cluster_match = re.search(r\"cluster\\('([^']+)'\\)\", query)\n",
    "    if not cluster_match:\n",
    "        raise ValueError(\"Could not extract cluster from query.\")\n",
    "    database_match = re.search(r\"database\\('([^']+)'\\)\", query)\n",
    "    if not database_match:\n",
    "        raise ValueError(\"Could not extract database from query.\")\n",
    "    cluster = \"https://\" + cluster_match.group(1)\n",
    "    database = database_match.group(1)\n",
    "    cleaned_query = re.sub(r\"cluster\\('([^']+)'\\)\\.database\\('([^']+)'\\)\\.\", \"\", query)\n",
    "    kcsb = KustoConnectionStringBuilder.with_az_cli_authentication(cluster)\n",
    "    client = KustoClient(kcsb)\n",
    "    response = client.execute(database, cleaned_query)\n",
    "    results = []\n",
    "    columns = [col.column_name for col in response.primary_results[0].columns]\n",
    "    for row in response.primary_results[0]:\n",
    "        results.append(dict(zip(columns, row)))\n",
    "    return results\n",
    "\n",
    "# ---------------------------\n",
    "# KQL Authentication Route (Using Azure CLI)\n",
    "# ---------------------------\n",
    "def check_az_cli_auth():\n",
    "    az_command = \"az.cmd\" if platform.system() == \"Windows\" else \"az\"\n",
    "    try:\n",
    "        result = subprocess.run([az_command, \"account\", \"get-access-token\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            return True, result.stdout\n",
    "        else:\n",
    "            return False, result.stderr\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "@app.route('/kql_auth', methods=['POST'])\n",
    "def kql_auth():\n",
    "    global kql_authenticated\n",
    "    success, message = check_az_cli_auth()\n",
    "    if success:\n",
    "        kql_authenticated = True\n",
    "        return jsonify({'status': 'Authenticated', 'message': message})\n",
    "    else:\n",
    "        kql_authenticated = False\n",
    "        return jsonify({'status': 'Not Authenticated', 'message': message})\n",
    "\n",
    "# ---------------------------\n",
    "# Route: Ingest KQL Results into Vector DB\n",
    "# ---------------------------\n",
    "@app.route('/ingest_kql', methods=['POST'])\n",
    "def ingest_kql():\n",
    "    query = request.form.get('query', '')\n",
    "    summary = request.form.get('summary', '')\n",
    "    text_to_index = f\"KQL Ingestion: Query: {query}\\nSummary: {summary}\"\n",
    "    message = add_document_to_vector_db(text_to_index, metadata={\"source\": \"KQL Ingestion\"})\n",
    "    return jsonify({'message': message})\n",
    "\n",
    "# ---------------------------\n",
    "# Vector DB Information Route\n",
    "# ---------------------------\n",
    "@app.route('/vector_info', methods=['GET'])\n",
    "def vector_info():\n",
    "    vector_db_dir = settings.get('vector_db_location', 'vector_db')\n",
    "    files_info = []\n",
    "    if os.path.exists(vector_db_dir):\n",
    "        for f in os.listdir(vector_db_dir):\n",
    "            file_path = os.path.join(vector_db_dir, f)\n",
    "            if os.path.isfile(file_path):\n",
    "                size = os.path.getsize(file_path)\n",
    "                files_info.append({\"filename\": f, \"size\": size})\n",
    "        status = \"Exists\"\n",
    "        count = vector_store.index.ntotal if vector_store is not None and hasattr(vector_store.index, 'ntotal') else \"Unknown\"\n",
    "    else:\n",
    "        status = \"Not Initialized\"\n",
    "        count = 0\n",
    "    return jsonify({\n",
    "        'vector_db_status': status,\n",
    "        'document_count': count,\n",
    "        'vector_db_location': os.path.abspath(vector_db_dir),\n",
    "        'files': files_info\n",
    "    })\n",
    "\n",
    "# ---------------------------\n",
    "# Main Routes\n",
    "# ---------------------------\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html', chat_history=chat_history.get(\"default\", []), kql_auth_status=(\"Authenticated\" if kql_authenticated else \"Not Authenticated\"))\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    prompt = request.form['prompt']\n",
    "    context = \"\"\n",
    "    if vector_store is not None:\n",
    "        retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        retrieved_docs = retriever.get_relevant_documents(prompt)\n",
    "        if retrieved_docs:\n",
    "            context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        else:\n",
    "            context = \"No relevant context found in vector DB.\"\n",
    "    response = send_request_to_tuned_model(prompt, context=context)\n",
    "    if response and 'choices' in response and response['choices']:\n",
    "        result = response['choices'][0]['message']['content']\n",
    "        chat_history.setdefault(\"default\", []).append({\"prompt\": prompt, \"response\": result})\n",
    "        return jsonify({'prompt': prompt, 'response': result})\n",
    "    else:\n",
    "        return jsonify({'error': 'Error communicating with tuned LM model.'})\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file part in the request.'})\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No file selected.'})\n",
    "    uploads_dir = 'uploads'\n",
    "    if not os.path.exists(uploads_dir):\n",
    "        os.makedirs(uploads_dir)\n",
    "    file_path = os.path.join(uploads_dir, file.filename)\n",
    "    file.save(file_path)\n",
    "    processed_content = process_uploaded_file(file_path)\n",
    "    index_message = add_document_to_vector_db(processed_content, metadata={\"source\": file.filename})\n",
    "    return jsonify({'file': file.filename, 'content': processed_content, 'message': index_message})\n",
    "\n",
    "@app.route('/upload_url', methods=['POST'])\n",
    "def upload_url():\n",
    "    url_input = request.form['url']\n",
    "    content = process_url_content(url_input)\n",
    "    index_message = add_document_to_vector_db(content, metadata={\"source\": url_input})\n",
    "    return jsonify({'url': url_input, 'content': content, 'message': index_message})\n",
    "\n",
    "@app.route('/kql', methods=['POST'])\n",
    "def kql():\n",
    "    query = request.form['query']\n",
    "    try:\n",
    "        results = execute_kql_query(query)\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            table_html = df.to_html(classes=\"table table-striped\", index=False)\n",
    "        else:\n",
    "            table_html = \"<p>No results returned.</p>\"\n",
    "        results_text = json.dumps(results, indent=2, default=str)\n",
    "        summary_response = send_request_to_tuned_model(\"Summarize the following KQL query results and provide numerical insights:\\n\" + results_text)\n",
    "        if summary_response and 'choices' in summary_response and summary_response['choices']:\n",
    "            summary = summary_response['choices'][0]['message']['content']\n",
    "        else:\n",
    "            summary = \"No summary available.\"\n",
    "        return jsonify({'query': query, 'table_html': table_html, 'summary': summary, 'results': results})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "\n",
    "@app.route('/settings', methods=['GET'])\n",
    "def settings_page():\n",
    "    return render_template('settings.html', kql_auth_status=(\"Authenticated\" if kql_authenticated else \"Not Authenticated\"))\n",
    "\n",
    "# ---------------------------\n",
    "# Auto-Open Browser (Optional)\n",
    "# ---------------------------\n",
    "def open_browser():\n",
    "    import webbrowser\n",
    "    webbrowser.open_new('http://127.0.0.1:5000/')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists('uploads'):\n",
    "        os.makedirs('uploads')\n",
    "    if not os.path.exists(settings['vector_db_location']):\n",
    "        os.makedirs(settings['vector_db_location'])\n",
    "    threading.Timer(1, open_browser).start()\n",
    "    app.run(port=5000, debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autogen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
